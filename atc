We thank the reviewers for taking the time and effort to review the manuscript. We appreciate all valuable comments and suggestions, which help us improve the quality of the manuscript and share it with the community.
Review A
A.1. The paper shows a toy example of using fewer time slots as a time optimization, showing the data here would more strongly back up the decision to use fewer time slots. 
Thanks for the wonderful advice. We have demonstrated the data in the attached file， including both data sorting and sampling result. The data indeed strongly back up the decision to use fewer time slots. We will add the figures and the corresponding explanation in the revised version. 
A.2. Would it make sense to use more time and get higher accuracy for the overall bandwidth allocation problem? I'm curious about how the goal of running in a few hours was decided.
This is a very good question. We agree with you that it makes sense to use more time and get higher accuracy. First, it is sufficient to achieve the optimal results with regard to the traffic distribution in most cloud networks, for example, Azure selects 15 hours of execution time for recomputing the billable bandwidth. Second, for the case of variable traffic distribution like assigning the workload to any available port during link failure, we may have a large benefit loss if the triggered optimization can't be solved within 10 hours. And we expect to shorten the time to achieve optimal results, which is the contribution of this paper. By using the accelerating approach proposed in this paper, we can solve the overall optimization problem within 20 minutes, but with an error of less than 5% compared to the optimal solution. We will explain this issue in the revised version. 
A3. Figure 6 doesn't contain information about the locations that any computations are being run.
We apologize for the confusion. We will redraw Figure 6 and clearly illustrate the operating procedures there.
A4. Information about what data was examined and how you made the observation is missing when verifying the congestion problem of backbone links.
We conduct experiments by using the traffic data in our production networks but assume no backbone capacity constraints. Then we calculate the utilization of the backbone links according to the scheduling. Surprisingly, we observe ∼10-30% of the backbone links are overloaded. In the worst case, maximum link utilization can be 800%. 
A5. Data in the paper appears to be from one week. Do you have any other data or anything to show that this was a "typical" week?
Thanks for your comment. In the granularity of days, we demonstrate the consistency like the results given in Table 3, Section 7.1. We do have the same result based on data from different weeks which is very large (Tbits). We will provide the analysis in the paper. 
A6. I think the eval would benefit from more of a deep-dive into the effect of each component.
Thanks for your interest. We have multiple results but omit due to the lack of space. We’d like to demonstrate the effect of each component and share more experience since EdgeCross has been used by the production cloud networks. 
Review B
B1. The novelty of the paper
As far as we know, we are the first work explaining the newly emerging demand of cloud customers, which has diverse requirements with ambiguous statement for cloud network capability, including not only determined latency, bandwidth, security, but also the statements like as low latency as possible, as low cost as possible, etc. (as depicted in Table 1) To meet their requirements, we need to innovatively re-design the cloud networking system. For example, upgrading the data plane to identify the specified traffic of certain applications or users, monitoring the networking service of these identified traffic in short time granularity, and achieving fast response to jitters. EdgeCross enhances the control plane, which addressed one of the hardest challenges, i.e. satisfying the requirements with low cost regarding to the high computation complexity of route decisions. 
The effectiveness of EdgeCross is critical because the customers will move to our cloud if their demands are satisfied, and leave otherwise. The effectiveness of EdgeCross have been verified by not only the results given in the paper but attracting a large number of customers after it is deployed online. We’d like to share the experience to the community. Multiple points are new i.e. 
(1) explaining new demand of cloud network customers which are interesting for researchers, and can motivate new research works. 
(2) presenting key insights of high computation complexity due to billion-level application-aware flows and joint-optimization of both peering links and backbone network links
(3) a series of effective optimization approaches to accelerate the route computation. 
(4) the demand for Routing Table Compression at control plane to verify the generated routes are not well addressed in literatures. 
Thanks for your comments, we will provide more detailed information about real-world production. 
B.2. Since you have already compared against the state-of-the-art (Fig. 11), it would be expected that all other experiments (Figs. 12, 13, 14) yield similar comparisons. 
Cascara aims to optimize cost in the first phase of EdgeCross for billable bandwidth. Thus we can only provide the comparison on cost saving.
B.3. It is recommended to consider more recent algorithms besides RadixTree, eg. LPM in DPDK.
Thanks for your comments. We focused on RadixTree (with continuous update) because it is widely used and stable in product environment. We have reviewed multiple works such as Tree BitMap [1], SAIL [2], and Poptrie [3]. ForestBitmap outperforms these results. The LPM in DPDK can't be applied in our system because it consumes large memory for performance. But we focus on saving the memory. 
B.4.  Fig. 12 poses a challenge as the reader cannot ascertain whether a 2ms reduction in latency is significant or not. 
The latency is reduced from 36ms to 34ms. We have to hide the exact values from the product network. 

